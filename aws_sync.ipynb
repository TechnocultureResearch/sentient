{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "- To provide a file sync experience(and eficciency) similar to `repo sync`, or `awscli s3 sync`, etc.\n",
    "- Allow files to be uploaded on downloaded **ONLY** if there is a difference in the file's loacal cs remote copy\n",
    "- Provide a pip package that wraps this sync access to s3 remote assets\n",
    "- Provide this as a convinience package for using remote assets in notebooks\n",
    "\n",
    "## Procedure\n",
    "- allow downloading or uploading files to an s3 bucket.\n",
    "- when asked to download a file from s3:\n",
    "  - check if the file is already present in the local directory\n",
    "  - if the file is present in the local directory:\n",
    "    - check if its hash matches the hash of the s3 version of this file\n",
    "    - if there is a mismatch in the hash of the file between the local and the s3 copy, it means that the contents of the file has changed\n",
    "    - if hashes mismatch: proceed with download\n",
    "- when asked to upload a file to s3:\n",
    "  - repeat the same process as involved in dowloading a file\n",
    "\n",
    "## Available appraoches\n",
    "- Option 1: wrap the command line call to `awscli s3 sync` using python `subprocess` library\n",
    "- Option 2: write the sync functionality from scratch âœ…\n",
    "\n",
    "## Dependencies\n",
    "- boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import shutil\n",
    "import boto3 as boto\n",
    "import multiprocessing\n",
    "import copy\n",
    "import hashlib\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from functools import wraps\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LocalObjectCache:\n",
    "    \"\"\"Provides a local cache of an S3 bucket on disk, with the ability to sync up to the latest version of all files\"\"\"\n",
    "    _DEFAULT_PATH = '/tmp/local_object_store/'\n",
    "    counter = 0\n",
    "\n",
    "    def __init__(self, bucket_name, prefix='', path=None):\n",
    "        \"\"\"Init Method\n",
    "        :param bucket_name: str, the name of the S3 bucket\n",
    "        :param prefix: str, the prefix up to which you want to sync\n",
    "        :param path: (optional, str) a path to store the local files\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "\n",
    "        if not path:\n",
    "            path = self._DEFAULT_PATH + self.bucket_name + '/'\n",
    "\n",
    "        self.path = path\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        s3 = boto.resource('s3')\n",
    "        self.bucket = s3.Bucket(self.bucket_name)\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Provides a context manager which will open but not sync, then delete the cache on exit\"\"\"\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"\"\"Provides a context manager which will open but not sync, then delete the cache on exit\"\"\"\n",
    "        self.close()\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # Require to pickle and un-pickle the self object between multiprocessing pools\n",
    "        out = copy.copy(self.__dict__)\n",
    "        out['bucket'] = None\n",
    "        return out\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        # Require to pickle and un-pickle the self object between multiprocessing pools\n",
    "        s3 = boto.resource('s3')\n",
    "        d['bucket'] = s3.Bucket(d['bucket_name'])\n",
    "        self.__dict__ = d\n",
    "\n",
    "    def get_path(self, key):\n",
    "        \"\"\"Returns the local file storage path for a given file key\"\"\"\n",
    "        return os.path.join(self.path, self.prefix, key)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_s3_etag(file, chunk_size=8 * 1024 * 1024):\n",
    "        \"\"\"Calculates the S3 custom e-tag (a specially formatted MD5 hash)\"\"\"\n",
    "        md5s = []\n",
    "        \n",
    "        while True:\n",
    "            data = file.read(chunk_size)\n",
    "            if not data:\n",
    "                break\n",
    "            md5s.append(hashlib.md5(data))\n",
    "\n",
    "        if len(md5s) == 1:\n",
    "            return '\"{}\"'.format(md5s[0].hexdigest())\n",
    "\n",
    "        digests = b''.join(m.digest() for m in md5s)\n",
    "        digests_md5 = hashlib.md5(digests)\n",
    "        return '\"{}-{}\"'.format(digests_md5.hexdigest(), len(md5s))\n",
    "\n",
    "    def _get_obj(self, key, tag):\n",
    "        \"\"\"Downloads an object at key to file path, checking to see if an existing file matches the current hash\"\"\"\n",
    "        path = os.path.join(self.path, key)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        dl_flag = True\n",
    "        try:\n",
    "            f = open(path, 'rb')\n",
    "            if tag == self.calculate_s3_etag(f):\n",
    "                log.info('Cache Hit')\n",
    "                dl_flag = False\n",
    "            f.close()\n",
    "        except FileNotFoundError as e:\n",
    "            pass\n",
    "\n",
    "        if dl_flag:\n",
    "            LocalObjectCache.counter +=1 \n",
    "            log.info('Cache Miss')\n",
    "            self.bucket.download_file(key, path)\n",
    "    \n",
    "    def _set_obj(self, key, path=None):\n",
    "        \"\"\"Uploads an object at key to aws, checking to see if an existing file matches the current hash\"\"\"\n",
    "        path = os.path.join(self.path, self.prefix+Path(key).name)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        dl_flag = True\n",
    "        try:\n",
    "            f = open(path, 'rb')\n",
    "            file = open(key, 'rb')\n",
    "            key_tag = self.calculate_s3_etag(file)\n",
    "            cache_tag = self.calculate_s3_etag(f)\n",
    "            if key_tag==cache_tag:\n",
    "                log.info('cache Hit')\n",
    "                dl_flag = False\n",
    "            file.close()\n",
    "            f.close()\n",
    "        except FileNotFoundError as e:\n",
    "            pass\n",
    "\n",
    "        if dl_flag:\n",
    "            log.info('Cache Miss')\n",
    "            self.bucket.upload_file(key, self.prefix+Path(key).name)\n",
    "            return self.calculate_s3_etag(open(key, 'rb'))\n",
    "\n",
    "    def set_bucket(self, bucket=None):\n",
    "        \"\"\"sets the bucket name.\"\"\"\n",
    "        if bucket is None:\n",
    "            return self.__getstate__()\n",
    "        else:\n",
    "            return self.__setstate__(bucket)\n",
    "\n",
    "    def sync(self):\n",
    "        \"\"\"Syncs the local and remote S3 copies\"\"\"\n",
    "        pool = multiprocessing.Pool()\n",
    "        keys = [(obj.key, obj.e_tag) for obj in self.bucket.objects.filter(Prefix=self.prefix)]\n",
    "        pool.starmap(self._get_obj, keys)\n",
    "    \n",
    "    def upload_obj(self, key, path=None):\n",
    "        return self._set_obj(key, path)\n",
    "    \n",
    "    def download_obj(self, key, tag):\n",
    "        return self._get_obj(key, tag)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Deletes all local files\"\"\"\n",
    "        shutil.rmtree(self.path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples for using above class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1\n",
    "upload_obj is called to upload files on the aws bucket from system, checks if the file exists on local cache with the same tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create object of the LocalObjectCache to initialize the bucket name and prefix.'''\n",
    "bucket_name = 'sentientx'\n",
    "prefix = 'EOD/'\n",
    "obj = LocalObjectCache(bucket_name, prefix)\n",
    "\n",
    "'''uploading a obj.'''\n",
    "__filename__ = 'feather_files/feather_512.fth'\n",
    "key = os.path.abspath(__filename__)\n",
    "obj.upload_obj(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2\n",
    "download_obj is called to download files on the system from aws bucket, checks if the file exists on local cache with the same tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create object of the LocalObjectCache to initialize the bucket name and prefix.'''\n",
    "bucket_name = 'sentientx'\n",
    "prefix = 'EOD/'\n",
    "obj = LocalObjectCache(bucket_name, prefix)\n",
    "\n",
    "'''Downloading a key from obj.'''\n",
    "__filename__ = 'feather_512.fth'\n",
    "key = os.path.join(obj.prefix, __filename__)\n",
    "obj.download_obj(key, 'hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3\n",
    "Syncing the data of local and remote s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create object of the LocalObjectCache to initialize the bucket name and prefix.'''\n",
    "bucket_name = 'sentientx'\n",
    "prefix = 'EOD/'\n",
    "obj = LocalObjectCache(bucket_name, prefix)\n",
    "\n",
    "\"\"\"Syncs the data of remote s3 bucket and local directory.\"\"\"\n",
    "obj.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4\n",
    "Reset the bucketname or get the current bucket name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create object of the LocalObjectCache to initialize the bucket name and prefix.'''\n",
    "bucket_name = 'sentientx'\n",
    "prefix = 'EOD/'\n",
    "obj = LocalObjectCache(bucket_name, prefix)\n",
    "\n",
    "\"\"\"It takes an argument bucket_name. If bucket name is given then it will change the bucket name else returns the current bucket name and set it at none.\"\"\"\n",
    "obj.set_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 5\n",
    "To delete the all files from the local cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create object of the LocalObjectCache to initialize the bucket name and prefix.'''\n",
    "bucket_name = 'sentientx'\n",
    "prefix = 'EOD/'\n",
    "obj = LocalObjectCache(bucket_name, prefix)\n",
    "\n",
    "\"\"\"Delete all files from the local cache.\"\"\"\n",
    "obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading and downloading 30 files on aws\n",
    "Here, we will upload and download the data of 30 companies and analysing their time consumption on uploading and downloading the data with or without cache. We had already upload and downloaded the data on aws bucket and local cache memory respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "'''making dictionary for storing time taken for uploading and downloading.'''\n",
    "\n",
    "time_dict = dict()\n",
    "time_dict['upload'] = dict()\n",
    "time_dict['download'] = dict()\n",
    "time_dict['upload']['without_cache'] = dict()\n",
    "time_dict['download']['with_cache'] = dict()\n",
    "time_dict['upload']['with_cache'] = dict()\n",
    "time_dict['download']['without_cache'] = dict()\n",
    "\n",
    "__file__ = 'feather_files'\n",
    "path = os.path.abspath(__file__)\n",
    "files = os.listdir(path)\n",
    "bucket = 'sentientx'\n",
    "prefix='EOD/'\n",
    "\n",
    "'''Creating object of for local cache.'''\n",
    "object = LocalObjectCache(bucket, prefix)\n",
    "\n",
    "'''creating session for direct upload and download.'''\n",
    "s4 = boto.client('s3')\n",
    "\n",
    "'''Pre download and upload the files.'''\n",
    "for file in files:\n",
    "    object._set_obj(os.path.abspath(os.path.join(__file__, file)))\n",
    "    object._get_obj(os.path.join(object.prefix, file), '')\n",
    "\n",
    "i=0\n",
    "for file in files:\n",
    "    i += 1\n",
    "    tag = object.calculate_s3_etag(open(os.path.abspath(os.path.join(__file__, file)), 'rb'))\n",
    "    \n",
    "    uploading_time = %timeit -q -n1 -o s4.upload_file(os.path.abspath(os.path.join(__file__, file)), bucket, os.path.join(prefix,file))\n",
    "    time_dict['upload']['without_cache'][i] = uploading_time.best\n",
    "    uploading_time = %timeit -q -n1 -o object._set_obj(os.path.abspath(os.path.join(__file__, file)))\n",
    "    time_dict['upload']['with_cache'][i] = uploading_time.best\n",
    "\n",
    "    downloading_time = %timeit -q -n1 -o s4.download_file(bucket , os.path.join(prefix, file), os.path.abspath(os.path.join(__file__, file)))\n",
    "    time_dict['download']['without_cache'][i] = downloading_time.best\n",
    "    tag = object.calculate_s3_etag(open(os.path.abspath(os.path.join(__file__, file)), 'rb'))\n",
    "    downloading_time = %timeit -q -n1 -o object._get_obj(os.path.join(object.prefix, file),tag )\n",
    "    time_dict['download']['with_cache'][i] = downloading_time.best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creating dataframe of the time_dict '''\n",
    "dic = {'upload' : pd.DataFrame(time_dict['upload']), 'download' : pd.DataFrame(time_dict['download'])}\n",
    "res = pd.concat(dic.values(),axis=1,keys=dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading a file multiple times of same data with different names.\n",
    "Let's check by downloading a file multiple times of same data by changing its name using cache and without cache. Checking the no. of times download function called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = 'feather_files'\n",
    "path = os.path.abspath(__file__)\n",
    "files = os.listdir(path)\n",
    "bucket_name = 'sentientx'\n",
    "prefix='EOD/'\n",
    "\n",
    "path1 = '/aws/download/'\n",
    "os.makedirs(path1, exist_ok=True)\n",
    "\n",
    "def counter(func):\n",
    "    @wraps(func)\n",
    "    def tmp(*args, **kwargs):\n",
    "        tmp.count += 1\n",
    "        return func(*args, **kwargs)\n",
    "    tmp.count = 0\n",
    "    return tmp\n",
    "\n",
    "\n",
    "@counter\n",
    "def upload(bucket_name, prefix, file):\n",
    "    s4 = boto.resource('s3')\n",
    "    bucket = s4.Bucket(bucket_name)\n",
    "    bucket.download_file(os.path.join(prefix, file), os.path.abspath(os.path.join(path1, str(i)+file)))\n",
    "\n",
    "'''Creating object of for local cache.'''\n",
    "object = LocalObjectCache(bucket_name, prefix)\n",
    "\n",
    "'''creating session for direct upload and download.'''\n",
    "for file in files:\n",
    "    for i in range(10):\n",
    "        upload(bucket_name, prefix, file)\n",
    "        new_file = os.path.abspath(os.path.join(path1, str(i)+file))\n",
    "        tag = object.calculate_s3_etag(open(new_file, 'rb'))\n",
    "        object._get_obj(os.path.join(object.prefix, file), tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "count = pd.DataFrame({'No. of files':[upload.count,object.counter]}, index=['without_cache', 'with_cache'])\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between upload time and download time of data with or without cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_in_upload = sum(res['upload']['without_cache']) - sum(res['upload']['with_cache']) \n",
    "difference_in_download = sum(res['download']['without_cache']) - sum(res['download']['with_cache'])\n",
    "percentage_difference_in_upload = difference_in_upload *100 / 100\n",
    "percentage_difference_in_download = difference_in_download *100 / 100\n",
    "standard_diffrence_in_upload = res['upload']['without_cache'].std() - res['upload']['with_cache'].std()\n",
    "standard_diffrence_in_download = res['download']['without_cache'].std() - res['download']['with_cache'].std()\n",
    "print(r'Uploading time of without cache is taking {:.2f}% ({:.4f})more time than Uploading time of with cache.'.format(percentage_difference_in_upload, standard_diffrence_in_upload))\n",
    "print(r'Downloading time of without cache is taking {:.2f}% ({:.4f})more time than Downloading time of with cache.'.format(percentage_difference_in_download, standard_diffrence_in_download))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the graph\n",
    "Let's plot the graph to visualize the comparison between the uploading and downloading time of files with or without cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot of download and upload time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    " \n",
    "bp = ax.boxplot(res, patch_artist = True,\n",
    "                notch ='True', vert = 0)\n",
    " \n",
    "colors = ['#ffcaab', '#3d271c', '#e4cfea', '#b8f9d9']\n",
    " \n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    " \n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set(color ='#f88e36',\n",
    "                linewidth = 1.5,\n",
    "                linestyle =\":\")\n",
    " \n",
    "for cap in bp['caps']:\n",
    "    cap.set(color ='#b8c8f9',\n",
    "            linewidth = 2)\n",
    " \n",
    "for median in bp['medians']:\n",
    "    median.set(color ='red',\n",
    "               linewidth = 3)\n",
    " \n",
    "for flier in bp['fliers']:\n",
    "    flier.set(marker ='D',\n",
    "              color ='#f9b8b8',\n",
    "              alpha = 0.5)\n",
    "     \n",
    "ax.set_yticklabels(['upload_without_cache', 'upload_without_cache',\n",
    "                    'download_with_cache', 'upload_with_cache'])\n",
    " \n",
    "plt.title(\"Time taken in downloading and upload data with or without cache\")\n",
    " \n",
    "ax.get_xaxis().tick_bottom()\n",
    "ax.get_yaxis().tick_left()\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineplot of download and upload time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.figure(figsize=(10,8))\n",
    "\n",
    "# Upload times\n",
    "res['upload'].plot()\n",
    "plt.xlabel('Number of rows')\n",
    "plt.ylabel('Upload time [s]')\n",
    "plt.title('Upload time data with or without cache')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "\n",
    "# Reading times\n",
    "res['download'].plot()\n",
    "plt.xlabel('Number of rows')\n",
    "plt.ylabel('Download time [s]')\n",
    "plt.title('Download Time data with or without cache')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x=count.index, y='No. of files', data=count)\n",
    "plt.title('No. of download files', fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "|Uploading|Downloading|\n",
    "|:---:|:---:|\n",
    "|Uploading time of without cache is taking 4.67% (0.0400)more time than Uploading time of with cache.|Downloading time of without cache is taking 3.69% (0.0136)more time than Downloading time of with cache.|\n",
    "|Uploads a file single time with cache and mupltiple times without cache if we upload the file of same data multiple times.|Downloads a file single time with cache and mupltiple times without cache if we download the file of same data multiple times.|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
