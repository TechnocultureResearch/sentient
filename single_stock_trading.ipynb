{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install finrl package"
      ],
      "metadata": {
        "id": "n8cysYqY9AvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/AI4Finance-LLC/FinRL-Library.git"
      ],
      "metadata": {
        "id": "DKENtYKrf44G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "BWs1XHGh9KF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import datetime\n",
        "import os\n",
        "from finrl import config\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
        "from finrl.meta.preprocessor.preprocessors import data_split\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "#from finrl.trade.backtest import BackTestStats, BaselineStats, BackTestPlot\n",
        "import sys\n",
        "sys.path.append(\"../FinRL-Library\")\n",
        "\n",
        "#Create folders for data, result metrics, and tensorboard logs\n",
        "\n",
        "if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
        "    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n",
        "if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
        "    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n",
        "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
        "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
        "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
        "    os.makedirs(\"./\" + config.RESULTS_DIR)"
      ],
      "metadata": {
        "id": "FBetnSKEhwDI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download **Apple stocks data** from Yahoo Finance api"
      ],
      "metadata": {
        "id": "lyN3KZ3X9Uxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = YahooDownloader(start_date = '2009-01-01',\n",
        "                          end_date = '2021-01-01',\n",
        "                          ticker_list = ['AAPL']).fetch_data()\n",
        "data_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "ekpNsObWjWLF",
        "outputId": "6ced786e-c07a-4a82-cffb-e793e661419c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "Shape of DataFrame:  (3021, 8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date      open      high       low     close      volume   tic  day\n",
              "0  2009-01-02  3.067143  3.251429  3.041429  2.767330   746015200  AAPL    4\n",
              "1  2009-01-05  3.327500  3.435000  3.311071  2.884122  1181608400  AAPL    0\n",
              "2  2009-01-06  3.426786  3.470357  3.299643  2.836552  1289310400  AAPL    1\n",
              "3  2009-01-07  3.278929  3.303571  3.223571  2.775258   753048800  AAPL    2\n",
              "4  2009-01-08  3.229643  3.326786  3.215714  2.826794   673500800  AAPL    3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b6d964c5-a45e-4158-bd30-ac7baf4a8661\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>3.067143</td>\n",
              "      <td>3.251429</td>\n",
              "      <td>3.041429</td>\n",
              "      <td>2.767330</td>\n",
              "      <td>746015200</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2009-01-05</td>\n",
              "      <td>3.327500</td>\n",
              "      <td>3.435000</td>\n",
              "      <td>3.311071</td>\n",
              "      <td>2.884122</td>\n",
              "      <td>1181608400</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2009-01-06</td>\n",
              "      <td>3.426786</td>\n",
              "      <td>3.470357</td>\n",
              "      <td>3.299643</td>\n",
              "      <td>2.836552</td>\n",
              "      <td>1289310400</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2009-01-07</td>\n",
              "      <td>3.278929</td>\n",
              "      <td>3.303571</td>\n",
              "      <td>3.223571</td>\n",
              "      <td>2.775258</td>\n",
              "      <td>753048800</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2009-01-08</td>\n",
              "      <td>3.229643</td>\n",
              "      <td>3.326786</td>\n",
              "      <td>3.215714</td>\n",
              "      <td>2.826794</td>\n",
              "      <td>673500800</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6d964c5-a45e-4158-bd30-ac7baf4a8661')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b6d964c5-a45e-4158-bd30-ac7baf4a8661 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b6d964c5-a45e-4158-bd30-ac7baf4a8661');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "NB1l_wxz9dnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## let’s store the technical indicator column names in config.py\n",
        "tech_indicator_list=config.INDICATORS\n",
        "print(tech_indicator_list)\n",
        "\n",
        "#Passing parameter to FeatureEngineer for adding technical indicators.\n",
        "fe = FeatureEngineer(\n",
        "                    use_technical_indicator=True,\n",
        "                    tech_indicator_list = tech_indicator_list,\n",
        "                    use_turbulence=False,\n",
        "                    user_defined_feature = False)\n",
        "\n",
        "data_df = fe.preprocess_data(data_df)\n",
        "data_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "1_imWHnYnuUn",
        "outputId": "8a013dc3-e699-4a0a-a9a6-aaa40991dd42"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['macd', 'boll_ub', 'boll_lb', 'rsi_30', 'cci_30', 'dx_30', 'close_30_sma', 'close_60_sma']\n",
            "Successfully added technical indicators\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date      open      high       low     close      volume   tic  day  \\\n",
              "0  2009-01-02  3.067143  3.251429  3.041429  2.767330   746015200  AAPL    4   \n",
              "1  2009-01-05  3.327500  3.435000  3.311071  2.884122  1181608400  AAPL    0   \n",
              "2  2009-01-06  3.426786  3.470357  3.299643  2.836552  1289310400  AAPL    1   \n",
              "3  2009-01-07  3.278929  3.303571  3.223571  2.775258   753048800  AAPL    2   \n",
              "4  2009-01-08  3.229643  3.326786  3.215714  2.826794   673500800  AAPL    3   \n",
              "\n",
              "       macd   boll_ub   boll_lb      rsi_30     cci_30       dx_30  \\\n",
              "0  0.000000  2.990895  2.660556  100.000000  66.666667  100.000000   \n",
              "1  0.002620  2.990895  2.660556  100.000000  66.666667  100.000000   \n",
              "2  0.001864  2.946794  2.711875   70.355611  46.823483  100.000000   \n",
              "3 -0.000739  2.925916  2.705715   50.429133 -29.722578   43.607834   \n",
              "4 -0.000087  2.913865  2.722157   60.227086  -9.062869   48.357918   \n",
              "\n",
              "   close_30_sma  close_60_sma  \n",
              "0      2.767330      2.767330  \n",
              "1      2.825726      2.825726  \n",
              "2      2.829335      2.829335  \n",
              "3      2.815815      2.815815  \n",
              "4      2.818011      2.818011  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5782bc2-559f-4cae-9a54-4de997ef733e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>boll_ub</th>\n",
              "      <th>boll_lb</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>close_30_sma</th>\n",
              "      <th>close_60_sma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-01-02</td>\n",
              "      <td>3.067143</td>\n",
              "      <td>3.251429</td>\n",
              "      <td>3.041429</td>\n",
              "      <td>2.767330</td>\n",
              "      <td>746015200</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.990895</td>\n",
              "      <td>2.660556</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>2.767330</td>\n",
              "      <td>2.767330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2009-01-05</td>\n",
              "      <td>3.327500</td>\n",
              "      <td>3.435000</td>\n",
              "      <td>3.311071</td>\n",
              "      <td>2.884122</td>\n",
              "      <td>1181608400</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0</td>\n",
              "      <td>0.002620</td>\n",
              "      <td>2.990895</td>\n",
              "      <td>2.660556</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>2.825726</td>\n",
              "      <td>2.825726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2009-01-06</td>\n",
              "      <td>3.426786</td>\n",
              "      <td>3.470357</td>\n",
              "      <td>3.299643</td>\n",
              "      <td>2.836552</td>\n",
              "      <td>1289310400</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>1</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>2.946794</td>\n",
              "      <td>2.711875</td>\n",
              "      <td>70.355611</td>\n",
              "      <td>46.823483</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>2.829335</td>\n",
              "      <td>2.829335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2009-01-07</td>\n",
              "      <td>3.278929</td>\n",
              "      <td>3.303571</td>\n",
              "      <td>3.223571</td>\n",
              "      <td>2.775258</td>\n",
              "      <td>753048800</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.000739</td>\n",
              "      <td>2.925916</td>\n",
              "      <td>2.705715</td>\n",
              "      <td>50.429133</td>\n",
              "      <td>-29.722578</td>\n",
              "      <td>43.607834</td>\n",
              "      <td>2.815815</td>\n",
              "      <td>2.815815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2009-01-08</td>\n",
              "      <td>3.229643</td>\n",
              "      <td>3.326786</td>\n",
              "      <td>3.215714</td>\n",
              "      <td>2.826794</td>\n",
              "      <td>673500800</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.000087</td>\n",
              "      <td>2.913865</td>\n",
              "      <td>2.722157</td>\n",
              "      <td>60.227086</td>\n",
              "      <td>-9.062869</td>\n",
              "      <td>48.357918</td>\n",
              "      <td>2.818011</td>\n",
              "      <td>2.818011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5782bc2-559f-4cae-9a54-4de997ef733e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b5782bc2-559f-4cae-9a54-4de997ef733e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b5782bc2-559f-4cae-9a54-4de997ef733e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trading Environment Building\n",
        "This environment is based on OpenAI Gym framework, which simulates hte live stock market data with real market data. Let’s split the dataset into train(2009-01-01 to 2018-12-31) and trade(2019-01-01 to 2020-09-30) datasets."
      ],
      "metadata": {
        "id": "a6x6-MvK92aH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting data"
      ],
      "metadata": {
        "id": "PTasCw_R-F8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = data_split(data_df, start = '2009-01-01', end = '2019-01-01')\n",
        "trade = data_split(data_df, start = '2019-01-01', end = '2021-01-01')"
      ],
      "metadata": {
        "id": "Y4krGAr-BinQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiate environment"
      ],
      "metadata": {
        "id": "yk5ubO7G-JyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(config.INDICATORS)*stock_dimension\n",
        "print(f\"Stock data Dimensions: {stock_dimension}, State Spaces: {state_space}\")\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100, \n",
        "    \"initial_amount\": 100000, \n",
        "    \"num_stock_shares\":[20],\n",
        "    \"sell_cost_pct\":[160.5],\n",
        "    \"buy_cost_pct\": [150.5], \n",
        "    \"state_space\": state_space, \n",
        "    \"stock_dim\": stock_dimension, \n",
        "    \"tech_indicator_list\": config.INDICATORS, \n",
        "    \"action_space\": stock_dimension, \n",
        "    \"reward_scaling\": 1e-4}\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)\n",
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5IBocXlB85N",
        "outputId": "0f8e8be2-599a-4ff3-86f9-0500fcb3ca3f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock data Dimensions: 1, State Spaces: 11\n",
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement DRL Algorithms\n",
        "FinRL library uses fine-tuned algorithms such as  DQN, DDPG, Multi-Agent DDPG, PPO, SAC, A2C, and TD3. The implementation of DRL algorithms are based on OpenAI and Stable Baselines   \n",
        "`agent = DRLAgent(env = env_train)`"
      ],
      "metadata": {
        "id": "Ih2-is-c-BxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on 5 different models\n",
        "We are going to see implementation in 5 different models provided by FinRL: A2C, DDPG, PPO, TD3, and SAC"
      ],
      "metadata": {
        "id": "ShQG5y6T-icJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: A2C"
      ],
      "metadata": {
        "id": "pC0ZfVCA-mZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Model: A2C\n",
        "agent = DRLAgent(env = env_train)\n",
        "A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0002}\n",
        "model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)\n",
        "trained_a2c = agent.train_model(model=model_a2c, \n",
        "                                tb_log_name='a2c',\n",
        "                                total_timesteps=50000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMfYdOjawedv",
        "outputId": "59223ee7-c070-4f3a-9c92-370e4326cbff"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0002}\n",
            "Using cuda device\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 359           |\n",
            "|    iterations         | 100           |\n",
            "|    time_elapsed       | 1             |\n",
            "|    total_timesteps    | 500           |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.42         |\n",
            "|    explained_variance | -9.68e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 99            |\n",
            "|    policy_loss        | -0.0723       |\n",
            "|    reward             | -0.0001252819 |\n",
            "|    std                | 1             |\n",
            "|    value_loss         | 0.00462       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 361          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 2            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.42        |\n",
            "|    explained_variance | -2.64e+03    |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | -0.0128      |\n",
            "|    reward             | 0.0003839241 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.00229      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 365           |\n",
            "|    iterations         | 300           |\n",
            "|    time_elapsed       | 4             |\n",
            "|    total_timesteps    | 1500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.42         |\n",
            "|    explained_variance | -156          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 299           |\n",
            "|    policy_loss        | -0.00528      |\n",
            "|    reward             | -0.0031178794 |\n",
            "|    std                | 1             |\n",
            "|    value_loss         | 0.000318      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 366          |\n",
            "|    iterations         | 400          |\n",
            "|    time_elapsed       | 5            |\n",
            "|    total_timesteps    | 2000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.43        |\n",
            "|    explained_variance | -32.9        |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 399          |\n",
            "|    policy_loss        | -0.04        |\n",
            "|    reward             | 0.0007921181 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.00165      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 367         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.43       |\n",
            "|    explained_variance | -80.9       |\n",
            "|    learning_rate      | 0.0002      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 0.116       |\n",
            "|    reward             | 0.007829913 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.00484     |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 364          |\n",
            "|    iterations         | 600          |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 3000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.43        |\n",
            "|    explained_variance | -183         |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 599          |\n",
            "|    policy_loss        | 0.0149       |\n",
            "|    reward             | 7.578611e-05 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.000271     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 365           |\n",
            "|    iterations         | 700           |\n",
            "|    time_elapsed       | 9             |\n",
            "|    total_timesteps    | 3500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.43         |\n",
            "|    explained_variance | -10.2         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 699           |\n",
            "|    policy_loss        | 0.015         |\n",
            "|    reward             | -0.0006912908 |\n",
            "|    std                | 1.01          |\n",
            "|    value_loss         | 0.000167      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 366           |\n",
            "|    iterations         | 800           |\n",
            "|    time_elapsed       | 10            |\n",
            "|    total_timesteps    | 4000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.44         |\n",
            "|    explained_variance | -64.5         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 799           |\n",
            "|    policy_loss        | 0.11          |\n",
            "|    reward             | -0.0017040911 |\n",
            "|    std                | 1.02          |\n",
            "|    value_loss         | 0.00724       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 367           |\n",
            "|    iterations         | 900           |\n",
            "|    time_elapsed       | 12            |\n",
            "|    total_timesteps    | 4500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.44         |\n",
            "|    explained_variance | -1.37e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 899           |\n",
            "|    policy_loss        | -0.0363       |\n",
            "|    reward             | 3.2797812e-05 |\n",
            "|    std                | 1.02          |\n",
            "|    value_loss         | 0.00139       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 367         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.44       |\n",
            "|    explained_variance | -19.3       |\n",
            "|    learning_rate      | 0.0002      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 0.141       |\n",
            "|    reward             | 0.001641119 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.0102      |\n",
            "---------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 368            |\n",
            "|    iterations         | 1100           |\n",
            "|    time_elapsed       | 14             |\n",
            "|    total_timesteps    | 5500           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.45          |\n",
            "|    explained_variance | -1.16e+03      |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 1099           |\n",
            "|    policy_loss        | -0.0027        |\n",
            "|    reward             | -0.00044003697 |\n",
            "|    std                | 1.03           |\n",
            "|    value_loss         | 0.000707       |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 368          |\n",
            "|    iterations         | 1200         |\n",
            "|    time_elapsed       | 16           |\n",
            "|    total_timesteps    | 6000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.45        |\n",
            "|    explained_variance | -207         |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 1199         |\n",
            "|    policy_loss        | 0.0627       |\n",
            "|    reward             | 0.0004011448 |\n",
            "|    std                | 1.03         |\n",
            "|    value_loss         | 0.00272      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 369          |\n",
            "|    iterations         | 1300         |\n",
            "|    time_elapsed       | 17           |\n",
            "|    total_timesteps    | 6500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.45        |\n",
            "|    explained_variance | -88.7        |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 1299         |\n",
            "|    policy_loss        | -0.0265      |\n",
            "|    reward             | -0.001694022 |\n",
            "|    std                | 1.03         |\n",
            "|    value_loss         | 0.00146      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 369         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 18          |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.46       |\n",
            "|    explained_variance | -3.16       |\n",
            "|    learning_rate      | 0.0002      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | -0.142      |\n",
            "|    reward             | 0.004589375 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 0.017       |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 369           |\n",
            "|    iterations         | 1500          |\n",
            "|    time_elapsed       | 20            |\n",
            "|    total_timesteps    | 7500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.46         |\n",
            "|    explained_variance | -11           |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 1499          |\n",
            "|    policy_loss        | 0.0474        |\n",
            "|    reward             | -0.0023883788 |\n",
            "|    std                | 1.04          |\n",
            "|    value_loss         | 0.00201       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 369         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 21          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.46       |\n",
            "|    explained_variance | -16         |\n",
            "|    learning_rate      | 0.0002      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 0.0868      |\n",
            "|    reward             | 0.001323472 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 0.00438     |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 369          |\n",
            "|    iterations         | 1700         |\n",
            "|    time_elapsed       | 23           |\n",
            "|    total_timesteps    | 8500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.47        |\n",
            "|    explained_variance | -6.24e+03    |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 1699         |\n",
            "|    policy_loss        | -0.228       |\n",
            "|    reward             | 0.0020007575 |\n",
            "|    std                | 1.05         |\n",
            "|    value_loss         | 0.0222       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 369            |\n",
            "|    iterations         | 1800           |\n",
            "|    time_elapsed       | 24             |\n",
            "|    total_timesteps    | 9000           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.47          |\n",
            "|    explained_variance | -8.55e+03      |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 1799           |\n",
            "|    policy_loss        | 0.127          |\n",
            "|    reward             | -0.00025181522 |\n",
            "|    std                | 1.05           |\n",
            "|    value_loss         | 0.0155         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 369           |\n",
            "|    iterations         | 1900          |\n",
            "|    time_elapsed       | 25            |\n",
            "|    total_timesteps    | 9500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.47         |\n",
            "|    explained_variance | -1.2e+03      |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 1899          |\n",
            "|    policy_loss        | 0.0456        |\n",
            "|    reward             | -0.0005707306 |\n",
            "|    std                | 1.06          |\n",
            "|    value_loss         | 0.0021        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 369           |\n",
            "|    iterations         | 2000          |\n",
            "|    time_elapsed       | 27            |\n",
            "|    total_timesteps    | 10000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.48         |\n",
            "|    explained_variance | -47           |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 1999          |\n",
            "|    policy_loss        | 0.0258        |\n",
            "|    reward             | -0.0009704132 |\n",
            "|    std                | 1.06          |\n",
            "|    value_loss         | 0.000622      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 369           |\n",
            "|    iterations         | 2100          |\n",
            "|    time_elapsed       | 28            |\n",
            "|    total_timesteps    | 10500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.48         |\n",
            "|    explained_variance | -42.4         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 2099          |\n",
            "|    policy_loss        | 0.0197        |\n",
            "|    reward             | 0.00010965548 |\n",
            "|    std                | 1.07          |\n",
            "|    value_loss         | 0.000301      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 368            |\n",
            "|    iterations         | 2200           |\n",
            "|    time_elapsed       | 29             |\n",
            "|    total_timesteps    | 11000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.49          |\n",
            "|    explained_variance | -83.7          |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 2199           |\n",
            "|    policy_loss        | 0.0387         |\n",
            "|    reward             | -1.8028068e-05 |\n",
            "|    std                | 1.07           |\n",
            "|    value_loss         | 0.000571       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 369            |\n",
            "|    iterations         | 2300           |\n",
            "|    time_elapsed       | 31             |\n",
            "|    total_timesteps    | 11500          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.49          |\n",
            "|    explained_variance | -3.3e+03       |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 2299           |\n",
            "|    policy_loss        | 0.0213         |\n",
            "|    reward             | -0.00050093076 |\n",
            "|    std                | 1.08           |\n",
            "|    value_loss         | 0.0014         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 369           |\n",
            "|    iterations         | 2400          |\n",
            "|    time_elapsed       | 32            |\n",
            "|    total_timesteps    | 12000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.5          |\n",
            "|    explained_variance | -7.18         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 2399          |\n",
            "|    policy_loss        | 0.000612      |\n",
            "|    reward             | -0.0016696155 |\n",
            "|    std                | 1.08          |\n",
            "|    value_loss         | 7.85e-06      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 369          |\n",
            "|    iterations         | 2500         |\n",
            "|    time_elapsed       | 33           |\n",
            "|    total_timesteps    | 12500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.5         |\n",
            "|    explained_variance | -12.9        |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 2499         |\n",
            "|    policy_loss        | -0.029       |\n",
            "|    reward             | 0.0028148266 |\n",
            "|    std                | 1.09         |\n",
            "|    value_loss         | 0.000417     |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 369            |\n",
            "|    iterations         | 2600           |\n",
            "|    time_elapsed       | 35             |\n",
            "|    total_timesteps    | 13000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.5           |\n",
            "|    explained_variance | -9.93e+03      |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 2599           |\n",
            "|    policy_loss        | -0.258         |\n",
            "|    reward             | -7.8549194e-05 |\n",
            "|    std                | 1.09           |\n",
            "|    value_loss         | 0.188          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 369           |\n",
            "|    iterations         | 2700          |\n",
            "|    time_elapsed       | 36            |\n",
            "|    total_timesteps    | 13500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.51         |\n",
            "|    explained_variance | -4.11e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 2699          |\n",
            "|    policy_loss        | 0.201         |\n",
            "|    reward             | -0.0005342285 |\n",
            "|    std                | 1.09          |\n",
            "|    value_loss         | 0.027         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 369            |\n",
            "|    iterations         | 2800           |\n",
            "|    time_elapsed       | 37             |\n",
            "|    total_timesteps    | 14000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.51          |\n",
            "|    explained_variance | -664           |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 2799           |\n",
            "|    policy_loss        | 0.052          |\n",
            "|    reward             | -6.9133755e-05 |\n",
            "|    std                | 1.09           |\n",
            "|    value_loss         | 0.00406        |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 370          |\n",
            "|    iterations         | 2900         |\n",
            "|    time_elapsed       | 39           |\n",
            "|    total_timesteps    | 14500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.51        |\n",
            "|    explained_variance | -9.64        |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 2899         |\n",
            "|    policy_loss        | 0.0365       |\n",
            "|    reward             | 4.570198e-05 |\n",
            "|    std                | 1.09         |\n",
            "|    value_loss         | 0.000673     |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 370            |\n",
            "|    iterations         | 3000           |\n",
            "|    time_elapsed       | 40             |\n",
            "|    total_timesteps    | 15000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.51          |\n",
            "|    explained_variance | -3.86e+03      |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 2999           |\n",
            "|    policy_loss        | 0.103          |\n",
            "|    reward             | -0.00029672775 |\n",
            "|    std                | 1.1            |\n",
            "|    value_loss         | 0.0228         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 370           |\n",
            "|    iterations         | 3100          |\n",
            "|    time_elapsed       | 41            |\n",
            "|    total_timesteps    | 15500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.52         |\n",
            "|    explained_variance | -133          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 3099          |\n",
            "|    policy_loss        | 0.00677       |\n",
            "|    reward             | 0.00042173482 |\n",
            "|    std                | 1.1           |\n",
            "|    value_loss         | 7.58e-05      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 370           |\n",
            "|    iterations         | 3200          |\n",
            "|    time_elapsed       | 43            |\n",
            "|    total_timesteps    | 16000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.52         |\n",
            "|    explained_variance | -110          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 3199          |\n",
            "|    policy_loss        | -0.00347      |\n",
            "|    reward             | -0.0003618164 |\n",
            "|    std                | 1.1           |\n",
            "|    value_loss         | 5.18e-05      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 370           |\n",
            "|    iterations         | 3300          |\n",
            "|    time_elapsed       | 44            |\n",
            "|    total_timesteps    | 16500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.52         |\n",
            "|    explained_variance | -53.1         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 3299          |\n",
            "|    policy_loss        | -0.0104       |\n",
            "|    reward             | -0.0001775465 |\n",
            "|    std                | 1.11          |\n",
            "|    value_loss         | 9.53e-05      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 370          |\n",
            "|    iterations         | 3400         |\n",
            "|    time_elapsed       | 45           |\n",
            "|    total_timesteps    | 17000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.53        |\n",
            "|    explained_variance | -352         |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 3399         |\n",
            "|    policy_loss        | 0.0454       |\n",
            "|    reward             | -0.002184471 |\n",
            "|    std                | 1.11         |\n",
            "|    value_loss         | 0.000559     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 370           |\n",
            "|    iterations         | 3500          |\n",
            "|    time_elapsed       | 47            |\n",
            "|    total_timesteps    | 17500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.54         |\n",
            "|    explained_variance | -23.1         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 3499          |\n",
            "|    policy_loss        | 0.00374       |\n",
            "|    reward             | -6.119728e-05 |\n",
            "|    std                | 1.12          |\n",
            "|    value_loss         | 0.000186      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 370            |\n",
            "|    iterations         | 3600           |\n",
            "|    time_elapsed       | 48             |\n",
            "|    total_timesteps    | 18000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.54          |\n",
            "|    explained_variance | -9.8e+03       |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 3599           |\n",
            "|    policy_loss        | -0.11          |\n",
            "|    reward             | -0.00026938933 |\n",
            "|    std                | 1.13           |\n",
            "|    value_loss         | 0.014          |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 3700         |\n",
            "|    time_elapsed       | 49           |\n",
            "|    total_timesteps    | 18500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.55        |\n",
            "|    explained_variance | -4.3e+03     |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 3699         |\n",
            "|    policy_loss        | 0.117        |\n",
            "|    reward             | 0.0002360775 |\n",
            "|    std                | 1.14         |\n",
            "|    value_loss         | 0.0054       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 3800          |\n",
            "|    time_elapsed       | 51            |\n",
            "|    total_timesteps    | 19000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.55         |\n",
            "|    explained_variance | -7.98e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 3799          |\n",
            "|    policy_loss        | 0.0951        |\n",
            "|    reward             | 0.00010943642 |\n",
            "|    std                | 1.14          |\n",
            "|    value_loss         | 0.0086        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 3900          |\n",
            "|    time_elapsed       | 52            |\n",
            "|    total_timesteps    | 19500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.55         |\n",
            "|    explained_variance | -2.76e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 3899          |\n",
            "|    policy_loss        | 0.162         |\n",
            "|    reward             | 0.00029219894 |\n",
            "|    std                | 1.14          |\n",
            "|    value_loss         | 0.00806       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 371            |\n",
            "|    iterations         | 4000           |\n",
            "|    time_elapsed       | 53             |\n",
            "|    total_timesteps    | 20000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.55          |\n",
            "|    explained_variance | -290           |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 3999           |\n",
            "|    policy_loss        | -0.0318        |\n",
            "|    reward             | -0.00045026396 |\n",
            "|    std                | 1.14           |\n",
            "|    value_loss         | 0.00394        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 4100          |\n",
            "|    time_elapsed       | 55            |\n",
            "|    total_timesteps    | 20500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.56         |\n",
            "|    explained_variance | -278          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 4099          |\n",
            "|    policy_loss        | -0.0183       |\n",
            "|    reward             | 0.00037875024 |\n",
            "|    std                | 1.15          |\n",
            "|    value_loss         | 0.000421      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 371            |\n",
            "|    iterations         | 4200           |\n",
            "|    time_elapsed       | 56             |\n",
            "|    total_timesteps    | 21000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.56          |\n",
            "|    explained_variance | -6.46          |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 4199           |\n",
            "|    policy_loss        | 0.000263       |\n",
            "|    reward             | -0.00026842003 |\n",
            "|    std                | 1.15           |\n",
            "|    value_loss         | 1.49e-05       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 4300          |\n",
            "|    time_elapsed       | 57            |\n",
            "|    total_timesteps    | 21500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.56         |\n",
            "|    explained_variance | -226          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 4299          |\n",
            "|    policy_loss        | -0.0108       |\n",
            "|    reward             | 0.00014470024 |\n",
            "|    std                | 1.16          |\n",
            "|    value_loss         | 0.000137      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 4400         |\n",
            "|    time_elapsed       | 59           |\n",
            "|    total_timesteps    | 22000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.57        |\n",
            "|    explained_variance | 0.12         |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 4399         |\n",
            "|    policy_loss        | -0.00169     |\n",
            "|    reward             | 0.0010268269 |\n",
            "|    std                | 1.16         |\n",
            "|    value_loss         | 3.58e-06     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 4500          |\n",
            "|    time_elapsed       | 60            |\n",
            "|    total_timesteps    | 22500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.58         |\n",
            "|    explained_variance | -26.5         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 4499          |\n",
            "|    policy_loss        | -0.0155       |\n",
            "|    reward             | -0.0008684074 |\n",
            "|    std                | 1.17          |\n",
            "|    value_loss         | 8.31e-05      |\n",
            "-----------------------------------------\n",
            "day: 2515, episode: 10\n",
            "begin_total_asset: 100762.50\n",
            "end_total_asset: -1734.19\n",
            "total_reward: -102496.69\n",
            "total_cost: 99821.81\n",
            "total_trades: 600\n",
            "Sharpe: -0.229\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 4600          |\n",
            "|    time_elapsed       | 61            |\n",
            "|    total_timesteps    | 23000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.58         |\n",
            "|    explained_variance | -320          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 4599          |\n",
            "|    policy_loss        | 0.00324       |\n",
            "|    reward             | 6.3278676e-05 |\n",
            "|    std                | 1.18          |\n",
            "|    value_loss         | 5.39e-05      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 4700          |\n",
            "|    time_elapsed       | 63            |\n",
            "|    total_timesteps    | 23500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.59         |\n",
            "|    explained_variance | -1.83         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 4699          |\n",
            "|    policy_loss        | -0.0186       |\n",
            "|    reward             | 0.00023098469 |\n",
            "|    std                | 1.18          |\n",
            "|    value_loss         | 0.000214      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 371            |\n",
            "|    iterations         | 4800           |\n",
            "|    time_elapsed       | 64             |\n",
            "|    total_timesteps    | 24000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.6           |\n",
            "|    explained_variance | -200           |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 4799           |\n",
            "|    policy_loss        | 0.00535        |\n",
            "|    reward             | -0.00054715155 |\n",
            "|    std                | 1.19           |\n",
            "|    value_loss         | 3.26e-05       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 4900          |\n",
            "|    time_elapsed       | 65            |\n",
            "|    total_timesteps    | 24500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.6          |\n",
            "|    explained_variance | -0.505        |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 4899          |\n",
            "|    policy_loss        | 0.000265      |\n",
            "|    reward             | -0.0006203318 |\n",
            "|    std                | 1.2           |\n",
            "|    value_loss         | 4.68e-06      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 5000         |\n",
            "|    time_elapsed       | 67           |\n",
            "|    total_timesteps    | 25000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.61        |\n",
            "|    explained_variance | -6.13        |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 4999         |\n",
            "|    policy_loss        | -0.0482      |\n",
            "|    reward             | 0.0002640438 |\n",
            "|    std                | 1.21         |\n",
            "|    value_loss         | 0.000971     |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 371            |\n",
            "|    iterations         | 5100           |\n",
            "|    time_elapsed       | 68             |\n",
            "|    total_timesteps    | 25500          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.62          |\n",
            "|    explained_variance | -83.3          |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 5099           |\n",
            "|    policy_loss        | -0.0104        |\n",
            "|    reward             | -0.00023146247 |\n",
            "|    std                | 1.22           |\n",
            "|    value_loss         | 8.78e-05       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 5200          |\n",
            "|    time_elapsed       | 69            |\n",
            "|    total_timesteps    | 26000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.62         |\n",
            "|    explained_variance | -302          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 5199          |\n",
            "|    policy_loss        | -0.0472       |\n",
            "|    reward             | 0.00038056754 |\n",
            "|    std                | 1.23          |\n",
            "|    value_loss         | 0.00137       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 371            |\n",
            "|    iterations         | 5300           |\n",
            "|    time_elapsed       | 71             |\n",
            "|    total_timesteps    | 26500          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.63          |\n",
            "|    explained_variance | -1.27e+03      |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 5299           |\n",
            "|    policy_loss        | 0.0228         |\n",
            "|    reward             | -0.00013229942 |\n",
            "|    std                | 1.24           |\n",
            "|    value_loss         | 0.000469       |\n",
            "------------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 371         |\n",
            "|    iterations         | 5400        |\n",
            "|    time_elapsed       | 72          |\n",
            "|    total_timesteps    | 27000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.64       |\n",
            "|    explained_variance | -48.6       |\n",
            "|    learning_rate      | 0.0002      |\n",
            "|    n_updates          | 5399        |\n",
            "|    policy_loss        | 0.0542      |\n",
            "|    reward             | 0.000504776 |\n",
            "|    std                | 1.24        |\n",
            "|    value_loss         | 0.00157     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 371         |\n",
            "|    iterations         | 5500        |\n",
            "|    time_elapsed       | 73          |\n",
            "|    total_timesteps    | 27500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.65       |\n",
            "|    explained_variance | -151        |\n",
            "|    learning_rate      | 0.0002      |\n",
            "|    n_updates          | 5499        |\n",
            "|    policy_loss        | 0.0137      |\n",
            "|    reward             | 0.005078293 |\n",
            "|    std                | 1.26        |\n",
            "|    value_loss         | 0.000141    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 5600         |\n",
            "|    time_elapsed       | 75           |\n",
            "|    total_timesteps    | 28000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.65        |\n",
            "|    explained_variance | 0.702        |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 5599         |\n",
            "|    policy_loss        | -0.0026      |\n",
            "|    reward             | 4.730587e-05 |\n",
            "|    std                | 1.26         |\n",
            "|    value_loss         | 3.43e-06     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 371         |\n",
            "|    iterations         | 5700        |\n",
            "|    time_elapsed       | 76          |\n",
            "|    total_timesteps    | 28500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.66       |\n",
            "|    explained_variance | -58.8       |\n",
            "|    learning_rate      | 0.0002      |\n",
            "|    n_updates          | 5699        |\n",
            "|    policy_loss        | -0.031      |\n",
            "|    reward             | 0.000321056 |\n",
            "|    std                | 1.27        |\n",
            "|    value_loss         | 0.00041     |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 5800          |\n",
            "|    time_elapsed       | 77            |\n",
            "|    total_timesteps    | 29000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.66         |\n",
            "|    explained_variance | -206          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 5799          |\n",
            "|    policy_loss        | -0.0276       |\n",
            "|    reward             | 4.4643402e-06 |\n",
            "|    std                | 1.27          |\n",
            "|    value_loss         | 0.000974      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 372           |\n",
            "|    iterations         | 5900          |\n",
            "|    time_elapsed       | 79            |\n",
            "|    total_timesteps    | 29500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.66         |\n",
            "|    explained_variance | -96.8         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 5899          |\n",
            "|    policy_loss        | -0.0343       |\n",
            "|    reward             | -0.0012241973 |\n",
            "|    std                | 1.27          |\n",
            "|    value_loss         | 0.000715      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 372          |\n",
            "|    iterations         | 6000         |\n",
            "|    time_elapsed       | 80           |\n",
            "|    total_timesteps    | 30000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.66        |\n",
            "|    explained_variance | -2.78e+03    |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 5999         |\n",
            "|    policy_loss        | 0.269        |\n",
            "|    reward             | 0.0020901002 |\n",
            "|    std                | 1.28         |\n",
            "|    value_loss         | 0.035        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 372           |\n",
            "|    iterations         | 6100          |\n",
            "|    time_elapsed       | 81            |\n",
            "|    total_timesteps    | 30500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.67         |\n",
            "|    explained_variance | -273          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 6099          |\n",
            "|    policy_loss        | 0.0395        |\n",
            "|    reward             | 0.00084604253 |\n",
            "|    std                | 1.28          |\n",
            "|    value_loss         | 0.00128       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 372           |\n",
            "|    iterations         | 6200          |\n",
            "|    time_elapsed       | 83            |\n",
            "|    total_timesteps    | 31000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.67         |\n",
            "|    explained_variance | -0.00817      |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 6199          |\n",
            "|    policy_loss        | -0.029        |\n",
            "|    reward             | -0.0048304712 |\n",
            "|    std                | 1.28          |\n",
            "|    value_loss         | 0.000239      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 372            |\n",
            "|    iterations         | 6300           |\n",
            "|    time_elapsed       | 84             |\n",
            "|    total_timesteps    | 31500          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.67          |\n",
            "|    explained_variance | -18.3          |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 6299           |\n",
            "|    policy_loss        | 0.0145         |\n",
            "|    reward             | -0.00066343346 |\n",
            "|    std                | 1.29           |\n",
            "|    value_loss         | 3.47e-05       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 372            |\n",
            "|    iterations         | 6400           |\n",
            "|    time_elapsed       | 85             |\n",
            "|    total_timesteps    | 32000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.68          |\n",
            "|    explained_variance | -3.41          |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 6399           |\n",
            "|    policy_loss        | -0.012         |\n",
            "|    reward             | -0.00011752739 |\n",
            "|    std                | 1.29           |\n",
            "|    value_loss         | 0.000227       |\n",
            "------------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 372         |\n",
            "|    iterations         | 6500        |\n",
            "|    time_elapsed       | 87          |\n",
            "|    total_timesteps    | 32500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.68       |\n",
            "|    explained_variance | 0.68        |\n",
            "|    learning_rate      | 0.0002      |\n",
            "|    n_updates          | 6499        |\n",
            "|    policy_loss        | 0.00247     |\n",
            "|    reward             | 0.000365831 |\n",
            "|    std                | 1.3         |\n",
            "|    value_loss         | 1.42e-05    |\n",
            "---------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 372            |\n",
            "|    iterations         | 6600           |\n",
            "|    time_elapsed       | 88             |\n",
            "|    total_timesteps    | 33000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.69          |\n",
            "|    explained_variance | -354           |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 6599           |\n",
            "|    policy_loss        | -0.0956        |\n",
            "|    reward             | -2.9273033e-06 |\n",
            "|    std                | 1.31           |\n",
            "|    value_loss         | 0.00532        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 372            |\n",
            "|    iterations         | 6700           |\n",
            "|    time_elapsed       | 90             |\n",
            "|    total_timesteps    | 33500          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.69          |\n",
            "|    explained_variance | -2.44e+03      |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 6699           |\n",
            "|    policy_loss        | 0.0492         |\n",
            "|    reward             | -3.6717225e-05 |\n",
            "|    std                | 1.31           |\n",
            "|    value_loss         | 0.00252        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 6800          |\n",
            "|    time_elapsed       | 91            |\n",
            "|    total_timesteps    | 34000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.69         |\n",
            "|    explained_variance | -1.28e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 6799          |\n",
            "|    policy_loss        | 0.0063        |\n",
            "|    reward             | 3.7437057e-05 |\n",
            "|    std                | 1.31          |\n",
            "|    value_loss         | 0.00307       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 371            |\n",
            "|    iterations         | 6900           |\n",
            "|    time_elapsed       | 92             |\n",
            "|    total_timesteps    | 34500          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.69          |\n",
            "|    explained_variance | -161           |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 6899           |\n",
            "|    policy_loss        | -0.0115        |\n",
            "|    reward             | -6.8226625e-05 |\n",
            "|    std                | 1.32           |\n",
            "|    value_loss         | 9.12e-05       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 7000          |\n",
            "|    time_elapsed       | 94            |\n",
            "|    total_timesteps    | 35000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.7          |\n",
            "|    explained_variance | -9.99e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 6999          |\n",
            "|    policy_loss        | -0.0279       |\n",
            "|    reward             | -9.037704e-05 |\n",
            "|    std                | 1.32          |\n",
            "|    value_loss         | 0.000315      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 7100         |\n",
            "|    time_elapsed       | 95           |\n",
            "|    total_timesteps    | 35500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.7         |\n",
            "|    explained_variance | -1.83e+03    |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 7099         |\n",
            "|    policy_loss        | 0.205        |\n",
            "|    reward             | 8.989658e-05 |\n",
            "|    std                | 1.33         |\n",
            "|    value_loss         | 0.02         |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 372          |\n",
            "|    iterations         | 7200         |\n",
            "|    time_elapsed       | 96           |\n",
            "|    total_timesteps    | 36000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.71        |\n",
            "|    explained_variance | -16.2        |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 7199         |\n",
            "|    policy_loss        | -0.101       |\n",
            "|    reward             | 1.945343e-05 |\n",
            "|    std                | 1.33         |\n",
            "|    value_loss         | 0.00486      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 372           |\n",
            "|    iterations         | 7300          |\n",
            "|    time_elapsed       | 98            |\n",
            "|    total_timesteps    | 36500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.71         |\n",
            "|    explained_variance | -1.03e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 7299          |\n",
            "|    policy_loss        | -0.0578       |\n",
            "|    reward             | 0.00039896945 |\n",
            "|    std                | 1.34          |\n",
            "|    value_loss         | 0.00307       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 7400         |\n",
            "|    time_elapsed       | 99           |\n",
            "|    total_timesteps    | 37000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.71        |\n",
            "|    explained_variance | -2.75e+03    |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 7399         |\n",
            "|    policy_loss        | 0.0193       |\n",
            "|    reward             | 0.0009986165 |\n",
            "|    std                | 1.33         |\n",
            "|    value_loss         | 0.00117      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 7500          |\n",
            "|    time_elapsed       | 100           |\n",
            "|    total_timesteps    | 37500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.71         |\n",
            "|    explained_variance | -2.25e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 7499          |\n",
            "|    policy_loss        | -0.0166       |\n",
            "|    reward             | -8.379745e-05 |\n",
            "|    std                | 1.34          |\n",
            "|    value_loss         | 0.000652      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 7600          |\n",
            "|    time_elapsed       | 102           |\n",
            "|    total_timesteps    | 38000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.72         |\n",
            "|    explained_variance | -44           |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 7599          |\n",
            "|    policy_loss        | -0.0176       |\n",
            "|    reward             | 5.2082632e-05 |\n",
            "|    std                | 1.35          |\n",
            "|    value_loss         | 0.000165      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 7700          |\n",
            "|    time_elapsed       | 103           |\n",
            "|    total_timesteps    | 38500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.72         |\n",
            "|    explained_variance | -2.36e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 7699          |\n",
            "|    policy_loss        | 0.0571        |\n",
            "|    reward             | 2.8602792e-05 |\n",
            "|    std                | 1.35          |\n",
            "|    value_loss         | 0.00184       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 371            |\n",
            "|    iterations         | 7800           |\n",
            "|    time_elapsed       | 104            |\n",
            "|    total_timesteps    | 39000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.72          |\n",
            "|    explained_variance | -111           |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 7799           |\n",
            "|    policy_loss        | 0.00623        |\n",
            "|    reward             | -0.00013023529 |\n",
            "|    std                | 1.36           |\n",
            "|    value_loss         | 0.000366       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 7900          |\n",
            "|    time_elapsed       | 106           |\n",
            "|    total_timesteps    | 39500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.73         |\n",
            "|    explained_variance | -193          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 7899          |\n",
            "|    policy_loss        | 0.037         |\n",
            "|    reward             | 0.00045574876 |\n",
            "|    std                | 1.37          |\n",
            "|    value_loss         | 0.000435      |\n",
            "-----------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 371      |\n",
            "|    iterations         | 8000     |\n",
            "|    time_elapsed       | 107      |\n",
            "|    total_timesteps    | 40000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.74    |\n",
            "|    explained_variance | -136     |\n",
            "|    learning_rate      | 0.0002   |\n",
            "|    n_updates          | 7999     |\n",
            "|    policy_loss        | 0.0474   |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 1.37     |\n",
            "|    value_loss         | 0.000538 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 8100          |\n",
            "|    time_elapsed       | 108           |\n",
            "|    total_timesteps    | 40500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.74         |\n",
            "|    explained_variance | -403          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 8099          |\n",
            "|    policy_loss        | 0.0111        |\n",
            "|    reward             | -0.0002561517 |\n",
            "|    std                | 1.37          |\n",
            "|    value_loss         | 0.000628      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 8200          |\n",
            "|    time_elapsed       | 110           |\n",
            "|    total_timesteps    | 41000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.74         |\n",
            "|    explained_variance | -282          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 8199          |\n",
            "|    policy_loss        | -0.0158       |\n",
            "|    reward             | 0.00078858377 |\n",
            "|    std                | 1.38          |\n",
            "|    value_loss         | 0.000186      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 8300          |\n",
            "|    time_elapsed       | 111           |\n",
            "|    total_timesteps    | 41500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.74         |\n",
            "|    explained_variance | -1.96e+03     |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 8299          |\n",
            "|    policy_loss        | -0.0215       |\n",
            "|    reward             | 0.00039644624 |\n",
            "|    std                | 1.38          |\n",
            "|    value_loss         | 0.000604      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 8400         |\n",
            "|    time_elapsed       | 112          |\n",
            "|    total_timesteps    | 42000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.75        |\n",
            "|    explained_variance | -105         |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 8399         |\n",
            "|    policy_loss        | 0.011        |\n",
            "|    reward             | 0.0005158138 |\n",
            "|    std                | 1.39         |\n",
            "|    value_loss         | 8.67e-05     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 8500          |\n",
            "|    time_elapsed       | 114           |\n",
            "|    total_timesteps    | 42500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.76         |\n",
            "|    explained_variance | -233          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 8499          |\n",
            "|    policy_loss        | -0.00311      |\n",
            "|    reward             | -0.0016930274 |\n",
            "|    std                | 1.4           |\n",
            "|    value_loss         | 0.000286      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 8600          |\n",
            "|    time_elapsed       | 115           |\n",
            "|    total_timesteps    | 43000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.76         |\n",
            "|    explained_variance | -0.59         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 8599          |\n",
            "|    policy_loss        | -0.0729       |\n",
            "|    reward             | 0.00071355654 |\n",
            "|    std                | 1.41          |\n",
            "|    value_loss         | 0.00137       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 8700         |\n",
            "|    time_elapsed       | 117          |\n",
            "|    total_timesteps    | 43500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.76        |\n",
            "|    explained_variance | -359         |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 8699         |\n",
            "|    policy_loss        | 0.0546       |\n",
            "|    reward             | 0.0011754013 |\n",
            "|    std                | 1.41         |\n",
            "|    value_loss         | 0.00162      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 8800         |\n",
            "|    time_elapsed       | 118          |\n",
            "|    total_timesteps    | 44000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.77        |\n",
            "|    explained_variance | -179         |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 8799         |\n",
            "|    policy_loss        | 0.0103       |\n",
            "|    reward             | 0.0013037242 |\n",
            "|    std                | 1.42         |\n",
            "|    value_loss         | 0.000389     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 8900         |\n",
            "|    time_elapsed       | 119          |\n",
            "|    total_timesteps    | 44500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.78        |\n",
            "|    explained_variance | -1.01        |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 8899         |\n",
            "|    policy_loss        | -0.000127    |\n",
            "|    reward             | 0.0005811577 |\n",
            "|    std                | 1.43         |\n",
            "|    value_loss         | 3.09e-06     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 9000          |\n",
            "|    time_elapsed       | 121           |\n",
            "|    total_timesteps    | 45000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.78         |\n",
            "|    explained_variance | -27.7         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 8999          |\n",
            "|    policy_loss        | 0.0804        |\n",
            "|    reward             | -0.0008637028 |\n",
            "|    std                | 1.44          |\n",
            "|    value_loss         | 0.00273       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 9100          |\n",
            "|    time_elapsed       | 122           |\n",
            "|    total_timesteps    | 45500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.78         |\n",
            "|    explained_variance | -32.7         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 9099          |\n",
            "|    policy_loss        | -0.0604       |\n",
            "|    reward             | -3.140855e-05 |\n",
            "|    std                | 1.43          |\n",
            "|    value_loss         | 0.000767      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 371            |\n",
            "|    iterations         | 9200           |\n",
            "|    time_elapsed       | 123            |\n",
            "|    total_timesteps    | 46000          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.78          |\n",
            "|    explained_variance | -3.09e+03      |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 9199           |\n",
            "|    policy_loss        | 0.041          |\n",
            "|    reward             | -3.9629936e-06 |\n",
            "|    std                | 1.44           |\n",
            "|    value_loss         | 0.0003         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 9300         |\n",
            "|    time_elapsed       | 125          |\n",
            "|    total_timesteps    | 46500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.79        |\n",
            "|    explained_variance | -2.3e+03     |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 9299         |\n",
            "|    policy_loss        | -0.0131      |\n",
            "|    reward             | 9.327888e-05 |\n",
            "|    std                | 1.45         |\n",
            "|    value_loss         | 8.58e-05     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 9400          |\n",
            "|    time_elapsed       | 126           |\n",
            "|    total_timesteps    | 47000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.8          |\n",
            "|    explained_variance | -952          |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 9399          |\n",
            "|    policy_loss        | -0.0238       |\n",
            "|    reward             | 1.1434555e-06 |\n",
            "|    std                | 1.46          |\n",
            "|    value_loss         | 0.000529      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 371            |\n",
            "|    iterations         | 9500           |\n",
            "|    time_elapsed       | 127            |\n",
            "|    total_timesteps    | 47500          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.8           |\n",
            "|    explained_variance | -9.59          |\n",
            "|    learning_rate      | 0.0002         |\n",
            "|    n_updates          | 9499           |\n",
            "|    policy_loss        | 0.0118         |\n",
            "|    reward             | -0.00034285165 |\n",
            "|    std                | 1.47           |\n",
            "|    value_loss         | 5.03e-05       |\n",
            "------------------------------------------\n",
            "day: 2515, episode: 20\n",
            "begin_total_asset: 100762.50\n",
            "end_total_asset: 0.53\n",
            "total_reward: -100761.97\n",
            "total_cost: 99814.36\n",
            "total_trades: 408\n",
            "Sharpe: 0.331\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 9600          |\n",
            "|    time_elapsed       | 129           |\n",
            "|    total_timesteps    | 48000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.81         |\n",
            "|    explained_variance | -25.2         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 9599          |\n",
            "|    policy_loss        | -0.00862      |\n",
            "|    reward             | 0.00010358415 |\n",
            "|    std                | 1.47          |\n",
            "|    value_loss         | 5.16e-05      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 9700         |\n",
            "|    time_elapsed       | 130          |\n",
            "|    total_timesteps    | 48500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.81        |\n",
            "|    explained_variance | -263         |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 9699         |\n",
            "|    policy_loss        | 0.00946      |\n",
            "|    reward             | 0.0001153924 |\n",
            "|    std                | 1.48         |\n",
            "|    value_loss         | 7.51e-05     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 371           |\n",
            "|    iterations         | 9800          |\n",
            "|    time_elapsed       | 131           |\n",
            "|    total_timesteps    | 49000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.81         |\n",
            "|    explained_variance | -3.36         |\n",
            "|    learning_rate      | 0.0002        |\n",
            "|    n_updates          | 9799          |\n",
            "|    policy_loss        | 0.0145        |\n",
            "|    reward             | 0.00082916114 |\n",
            "|    std                | 1.49          |\n",
            "|    value_loss         | 9.66e-05      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 9900         |\n",
            "|    time_elapsed       | 133          |\n",
            "|    total_timesteps    | 49500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.82        |\n",
            "|    explained_variance | -4.07        |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 9899         |\n",
            "|    policy_loss        | -0.106       |\n",
            "|    reward             | 0.0033177272 |\n",
            "|    std                | 1.5          |\n",
            "|    value_loss         | 0.00145      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 371          |\n",
            "|    iterations         | 10000        |\n",
            "|    time_elapsed       | 134          |\n",
            "|    total_timesteps    | 50000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.83        |\n",
            "|    explained_variance | -14.4        |\n",
            "|    learning_rate      | 0.0002       |\n",
            "|    n_updates          | 9999         |\n",
            "|    policy_loss        | 0.00111      |\n",
            "|    reward             | 0.0015303692 |\n",
            "|    std                | 1.5          |\n",
            "|    value_loss         | 2.79e-05     |\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: DDPG"
      ],
      "metadata": {
        "id": "_v0wUzia-rSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Model: DDPG\n",
        "agent = DRLAgent(env = env_train)\n",
        "DDPG_PARAMS = {\"batch_size\": 64, \"buffer_size\": 500000, \"learning_rate\": 0.0001}\n",
        "model_ddpg = agent.get_model(\"ddpg\",model_kwargs = DDPG_PARAMS)\n",
        "\n",
        "trained_ddpg = agent.train_model(model=model_ddpg, \n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=30000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYLbLek3weLo",
        "outputId": "4bad847b-b5f9-42eb-b12e-ff4bab7a4f13"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 64, 'buffer_size': 500000, 'learning_rate': 0.0001}\n",
            "Using cuda device\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 99       |\n",
            "|    time_elapsed    | 101      |\n",
            "|    total_timesteps | 10064    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -155     |\n",
            "|    critic_loss     | 649      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 7548     |\n",
            "|    reward          | 0.0      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 85       |\n",
            "|    time_elapsed    | 234      |\n",
            "|    total_timesteps | 20128    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.8    |\n",
            "|    critic_loss     | 376      |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 17612    |\n",
            "|    reward          | 0.0      |\n",
            "---------------------------------\n",
            "day: 2515, episode: 30\n",
            "begin_total_asset: 100762.50\n",
            "end_total_asset: 862.32\n",
            "total_reward: -99900.18\n",
            "total_cost: 99278.64\n",
            "total_trades: 2515\n",
            "Sharpe: 0.058\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/              |              |\n",
            "|    episodes        | 12           |\n",
            "|    fps             | 82           |\n",
            "|    time_elapsed    | 365          |\n",
            "|    total_timesteps | 30192        |\n",
            "| train/             |              |\n",
            "|    actor_loss      | -15.3        |\n",
            "|    critic_loss     | 325          |\n",
            "|    learning_rate   | 0.0001       |\n",
            "|    n_updates       | 27676        |\n",
            "|    reward          | 0.0007663914 |\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: PPO"
      ],
      "metadata": {
        "id": "lERQSMZp-zfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Model: PPO\n",
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.005,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"batch_size\": 128,}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "trained_ppo = agent.train_model(model=model_ppo, \n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=80000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NWvYLbHweA2",
        "outputId": "af36603d-f253-49d5-ad66-3443467f6a6c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 128}\n",
            "Using cuda device\n",
            "---------------------------------------\n",
            "| time/              |                |\n",
            "|    fps             | 557            |\n",
            "|    iterations      | 1              |\n",
            "|    time_elapsed    | 3              |\n",
            "|    total_timesteps | 2048           |\n",
            "| train/             |                |\n",
            "|    reward          | -0.00027984008 |\n",
            "---------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 505           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 8             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0001815393  |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.3          |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.00872       |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000618     |\n",
            "|    reward               | -0.0002498209 |\n",
            "|    std                  | 0.999         |\n",
            "|    value_loss           | 0.136         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 487          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.00336818   |\n",
            "|    clip_fraction        | 0.004        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -0.963       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.00256      |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00113     |\n",
            "|    reward               | 0.0023287197 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 0.123        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 479           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0050800703  |\n",
            "|    clip_fraction        | 0.0294        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0.062         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.153         |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.00402      |\n",
            "|    reward               | -0.0003647073 |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 0.209         |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 476            |\n",
            "|    iterations           | 5              |\n",
            "|    time_elapsed         | 21             |\n",
            "|    total_timesteps      | 10240          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0019759755   |\n",
            "|    clip_fraction        | 0.00239        |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.42          |\n",
            "|    explained_variance   | 0.186          |\n",
            "|    learning_rate        | 0.0001         |\n",
            "|    loss                 | -0.00272       |\n",
            "|    n_updates            | 40             |\n",
            "|    policy_gradient_loss | -0.0018        |\n",
            "|    reward               | -0.00096166093 |\n",
            "|    std                  | 1              |\n",
            "|    value_loss           | 0.0531         |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 459          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039097453 |\n",
            "|    clip_fraction        | 0.0303       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0.247        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 1.07e-05     |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00353     |\n",
            "|    reward               | 0.011171553  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 0.221        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 459          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 31           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032608989 |\n",
            "|    clip_fraction        | 0.0297       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -2.85        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | -0.0214      |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.00446     |\n",
            "|    reward               | -0.006675246 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 0.00485      |\n",
            "------------------------------------------\n",
            "day: 2515, episode: 40\n",
            "begin_total_asset: 100762.50\n",
            "end_total_asset: -7798.69\n",
            "total_reward: -108561.19\n",
            "total_cost: 100278.35\n",
            "total_trades: 1458\n",
            "Sharpe: -0.157\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 459          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 35           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046126503 |\n",
            "|    clip_fraction        | 0.0274       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0.522        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.159        |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00238     |\n",
            "|    reward               | 2.094841e-05 |\n",
            "|    std                  | 0.996        |\n",
            "|    value_loss           | 0.0962       |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 460           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 40            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0016987997  |\n",
            "|    clip_fraction        | 0.00605       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | 0.474         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.00627      |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.00208      |\n",
            "|    reward               | 0.00036665954 |\n",
            "|    std                  | 0.994         |\n",
            "|    value_loss           | 0.0776        |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 460          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.003311777  |\n",
            "|    clip_fraction        | 0.00864      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0.68         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.161        |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00261     |\n",
            "|    reward               | -0.001070716 |\n",
            "|    std                  | 0.999        |\n",
            "|    value_loss           | 0.046        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 456           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 49            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00090899144 |\n",
            "|    clip_fraction        | 0.00083       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0.23          |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.226         |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.00258      |\n",
            "|    reward               | -0.000492482  |\n",
            "|    std                  | 0.998         |\n",
            "|    value_loss           | 0.0961        |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 440          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 55           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043644207 |\n",
            "|    clip_fraction        | 0.0258       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -3.72        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | -0.00499     |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00467     |\n",
            "|    reward               | 9.095002e-06 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 0.0112       |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 442           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 60            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0028721613  |\n",
            "|    clip_fraction        | 0.0189        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0.242         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.174         |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.00431      |\n",
            "|    reward               | 5.4319382e-05 |\n",
            "|    std                  | 0.996         |\n",
            "|    value_loss           | 0.111         |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 443           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 64            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0023107356  |\n",
            "|    clip_fraction        | 0.00625       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0.411         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.00713      |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.0027       |\n",
            "|    reward               | -0.0051452033 |\n",
            "|    std                  | 0.998         |\n",
            "|    value_loss           | 0.0487        |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 445           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 69            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0014358128  |\n",
            "|    clip_fraction        | 0.00122       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0.135         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.0019        |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.00187      |\n",
            "|    reward               | -0.0003773328 |\n",
            "|    std                  | 0.998         |\n",
            "|    value_loss           | 0.087         |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 446            |\n",
            "|    iterations           | 16             |\n",
            "|    time_elapsed         | 73             |\n",
            "|    total_timesteps      | 32768          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.003861308    |\n",
            "|    clip_fraction        | 0.018          |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.42          |\n",
            "|    explained_variance   | 0.286          |\n",
            "|    learning_rate        | 0.0001         |\n",
            "|    loss                 | 0.00177        |\n",
            "|    n_updates            | 150            |\n",
            "|    policy_gradient_loss | -0.00459       |\n",
            "|    reward               | -4.2266416e-05 |\n",
            "|    std                  | 0.997          |\n",
            "|    value_loss           | 0.0582         |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 446           |\n",
            "|    iterations           | 17            |\n",
            "|    time_elapsed         | 77            |\n",
            "|    total_timesteps      | 34816         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0018941651  |\n",
            "|    clip_fraction        | 0.0085        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0.157         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.234         |\n",
            "|    n_updates            | 160           |\n",
            "|    policy_gradient_loss | -0.00413      |\n",
            "|    reward               | -0.0011906456 |\n",
            "|    std                  | 0.997         |\n",
            "|    value_loss           | 0.0424        |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 447           |\n",
            "|    iterations           | 18            |\n",
            "|    time_elapsed         | 82            |\n",
            "|    total_timesteps      | 36864         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0007818551  |\n",
            "|    clip_fraction        | 0.00142       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -0.617        |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.00439      |\n",
            "|    n_updates            | 170           |\n",
            "|    policy_gradient_loss | -0.00174      |\n",
            "|    reward               | -0.0016777486 |\n",
            "|    std                  | 0.997         |\n",
            "|    value_loss           | 0.00707       |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 448            |\n",
            "|    iterations           | 19             |\n",
            "|    time_elapsed         | 86             |\n",
            "|    total_timesteps      | 38912          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0063227676   |\n",
            "|    clip_fraction        | 0.0489         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.41          |\n",
            "|    explained_variance   | 0.373          |\n",
            "|    learning_rate        | 0.0001         |\n",
            "|    loss                 | -0.0148        |\n",
            "|    n_updates            | 180            |\n",
            "|    policy_gradient_loss | -0.00461       |\n",
            "|    reward               | -0.00019315796 |\n",
            "|    std                  | 0.989          |\n",
            "|    value_loss           | 0.0411         |\n",
            "--------------------------------------------\n",
            "day: 2515, episode: 50\n",
            "begin_total_asset: 100762.50\n",
            "end_total_asset: -2642.88\n",
            "total_reward: -103405.37\n",
            "total_cost: 99931.21\n",
            "total_trades: 1472\n",
            "Sharpe: -0.309\n",
            "=================================\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 449            |\n",
            "|    iterations           | 20             |\n",
            "|    time_elapsed         | 91             |\n",
            "|    total_timesteps      | 40960          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0020109098   |\n",
            "|    clip_fraction        | 0.00615        |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.41          |\n",
            "|    explained_variance   | 0.36           |\n",
            "|    learning_rate        | 0.0001         |\n",
            "|    loss                 | -0.00791       |\n",
            "|    n_updates            | 190            |\n",
            "|    policy_gradient_loss | -0.00318       |\n",
            "|    reward               | -0.00070672424 |\n",
            "|    std                  | 0.992          |\n",
            "|    value_loss           | 0.106          |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 449           |\n",
            "|    iterations           | 21            |\n",
            "|    time_elapsed         | 95            |\n",
            "|    total_timesteps      | 43008         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0020119203  |\n",
            "|    clip_fraction        | 0.00352       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | 0.494         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.00691       |\n",
            "|    n_updates            | 200           |\n",
            "|    policy_gradient_loss | -0.00231      |\n",
            "|    reward               | -0.0003627248 |\n",
            "|    std                  | 0.996         |\n",
            "|    value_loss           | 0.0813        |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 450           |\n",
            "|    iterations           | 22            |\n",
            "|    time_elapsed         | 100           |\n",
            "|    total_timesteps      | 45056         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0009666687  |\n",
            "|    clip_fraction        | 0.000342      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | 0.217         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.0113       |\n",
            "|    n_updates            | 210           |\n",
            "|    policy_gradient_loss | -0.00175      |\n",
            "|    reward               | 0.00035361672 |\n",
            "|    std                  | 0.994         |\n",
            "|    value_loss           | 0.0394        |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 450           |\n",
            "|    iterations           | 23            |\n",
            "|    time_elapsed         | 104           |\n",
            "|    total_timesteps      | 47104         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.001089162   |\n",
            "|    clip_fraction        | 0.002         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | -2.09         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.00627      |\n",
            "|    n_updates            | 220           |\n",
            "|    policy_gradient_loss | -0.00188      |\n",
            "|    reward               | -0.0022777268 |\n",
            "|    std                  | 0.994         |\n",
            "|    value_loss           | 0.00546       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 451           |\n",
            "|    iterations           | 24            |\n",
            "|    time_elapsed         | 108           |\n",
            "|    total_timesteps      | 49152         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0019616988  |\n",
            "|    clip_fraction        | 0.00239       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | 0.585         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.00741      |\n",
            "|    n_updates            | 230           |\n",
            "|    policy_gradient_loss | -0.00147      |\n",
            "|    reward               | -7.418156e-05 |\n",
            "|    std                  | 0.993         |\n",
            "|    value_loss           | 0.0594        |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 451           |\n",
            "|    iterations           | 25            |\n",
            "|    time_elapsed         | 113           |\n",
            "|    total_timesteps      | 51200         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0017329382  |\n",
            "|    clip_fraction        | 0.00479       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | 0.0162        |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.00049      |\n",
            "|    n_updates            | 240           |\n",
            "|    policy_gradient_loss | -0.00207      |\n",
            "|    reward               | -0.0011397171 |\n",
            "|    std                  | 0.994         |\n",
            "|    value_loss           | 0.0382        |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 451           |\n",
            "|    iterations           | 26            |\n",
            "|    time_elapsed         | 117           |\n",
            "|    total_timesteps      | 53248         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0030020708  |\n",
            "|    clip_fraction        | 0.0134        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | 0.349         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.0103       |\n",
            "|    n_updates            | 250           |\n",
            "|    policy_gradient_loss | -0.0042       |\n",
            "|    reward               | 0.00025761212 |\n",
            "|    std                  | 0.99          |\n",
            "|    value_loss           | 0.0361        |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 452         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 122         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003502511 |\n",
            "|    clip_fraction        | 0.0149      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.347       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.019      |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00229    |\n",
            "|    reward               | 0.00556901  |\n",
            "|    std                  | 0.992       |\n",
            "|    value_loss           | 0.0331      |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 452           |\n",
            "|    iterations           | 28            |\n",
            "|    time_elapsed         | 126           |\n",
            "|    total_timesteps      | 57344         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0045031304  |\n",
            "|    clip_fraction        | 0.0242        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | 0.299         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.000226      |\n",
            "|    n_updates            | 270           |\n",
            "|    policy_gradient_loss | -0.00297      |\n",
            "|    reward               | 0.00033043957 |\n",
            "|    std                  | 0.993         |\n",
            "|    value_loss           | 0.00109       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 453           |\n",
            "|    iterations           | 29            |\n",
            "|    time_elapsed         | 131           |\n",
            "|    total_timesteps      | 59392         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0028896022  |\n",
            "|    clip_fraction        | 0.00703       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | 0.297         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.0125       |\n",
            "|    n_updates            | 280           |\n",
            "|    policy_gradient_loss | -0.00207      |\n",
            "|    reward               | -0.0006798439 |\n",
            "|    std                  | 0.992         |\n",
            "|    value_loss           | 0.0355        |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 453          |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 135          |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016744725 |\n",
            "|    clip_fraction        | 0.00366      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.412        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | -0.017       |\n",
            "|    n_updates            | 290          |\n",
            "|    policy_gradient_loss | -0.00155     |\n",
            "|    reward               | -0.001277743 |\n",
            "|    std                  | 0.992        |\n",
            "|    value_loss           | 0.0607       |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 453            |\n",
            "|    iterations           | 31             |\n",
            "|    time_elapsed         | 139            |\n",
            "|    total_timesteps      | 63488          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0015489204   |\n",
            "|    clip_fraction        | 0.00737        |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.41          |\n",
            "|    explained_variance   | 0.569          |\n",
            "|    learning_rate        | 0.0001         |\n",
            "|    loss                 | -0.0115        |\n",
            "|    n_updates            | 300            |\n",
            "|    policy_gradient_loss | -0.00214       |\n",
            "|    reward               | -5.8485984e-05 |\n",
            "|    std                  | 0.991          |\n",
            "|    value_loss           | 0.0563         |\n",
            "--------------------------------------------\n",
            "day: 2515, episode: 60\n",
            "begin_total_asset: 100762.50\n",
            "end_total_asset: -419.95\n",
            "total_reward: -101182.45\n",
            "total_cost: 99325.88\n",
            "total_trades: 1693\n",
            "Sharpe: -0.045\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 454           |\n",
            "|    iterations           | 32            |\n",
            "|    time_elapsed         | 144           |\n",
            "|    total_timesteps      | 65536         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0015699633  |\n",
            "|    clip_fraction        | 0.00327       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | 0.278         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.015        |\n",
            "|    n_updates            | 310           |\n",
            "|    policy_gradient_loss | -0.00307      |\n",
            "|    reward               | -7.769837e-05 |\n",
            "|    std                  | 0.99          |\n",
            "|    value_loss           | 0.0332        |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 454            |\n",
            "|    iterations           | 33             |\n",
            "|    time_elapsed         | 148            |\n",
            "|    total_timesteps      | 67584          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0028770156   |\n",
            "|    clip_fraction        | 0.0132         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.41          |\n",
            "|    explained_variance   | 0.31           |\n",
            "|    learning_rate        | 0.0001         |\n",
            "|    loss                 | -0.0272        |\n",
            "|    n_updates            | 320            |\n",
            "|    policy_gradient_loss | -0.00554       |\n",
            "|    reward               | -0.00039362183 |\n",
            "|    std                  | 0.99           |\n",
            "|    value_loss           | 0.0355         |\n",
            "--------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 454            |\n",
            "|    iterations           | 34             |\n",
            "|    time_elapsed         | 153            |\n",
            "|    total_timesteps      | 69632          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0023045111   |\n",
            "|    clip_fraction        | 0.00664        |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.41          |\n",
            "|    explained_variance   | -2.27          |\n",
            "|    learning_rate        | 0.0001         |\n",
            "|    loss                 | -0.0216        |\n",
            "|    n_updates            | 330            |\n",
            "|    policy_gradient_loss | -0.00361       |\n",
            "|    reward               | -0.00020087471 |\n",
            "|    std                  | 0.997          |\n",
            "|    value_loss           | 0.00257        |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 454          |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 157          |\n",
            "|    total_timesteps      | 71680        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026568864 |\n",
            "|    clip_fraction        | 0.0131       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0.256        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | -0.0146      |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | -0.00344     |\n",
            "|    reward               | 9.719181e-05 |\n",
            "|    std                  | 0.998        |\n",
            "|    value_loss           | 0.0356       |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 454           |\n",
            "|    iterations           | 36            |\n",
            "|    time_elapsed         | 162           |\n",
            "|    total_timesteps      | 73728         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0025009094  |\n",
            "|    clip_fraction        | 0.0155        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0.324         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.00967      |\n",
            "|    n_updates            | 350           |\n",
            "|    policy_gradient_loss | -0.00389      |\n",
            "|    reward               | 0.00040954305 |\n",
            "|    std                  | 0.996         |\n",
            "|    value_loss           | 0.0343        |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 454            |\n",
            "|    iterations           | 37             |\n",
            "|    time_elapsed         | 166            |\n",
            "|    total_timesteps      | 75776          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0033804872   |\n",
            "|    clip_fraction        | 0.0211         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.42          |\n",
            "|    explained_variance   | 0.411          |\n",
            "|    learning_rate        | 0.0001         |\n",
            "|    loss                 | -0.00139       |\n",
            "|    n_updates            | 360            |\n",
            "|    policy_gradient_loss | -0.00488       |\n",
            "|    reward               | -0.00027633095 |\n",
            "|    std                  | 1              |\n",
            "|    value_loss           | 0.0889         |\n",
            "--------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 454            |\n",
            "|    iterations           | 38             |\n",
            "|    time_elapsed         | 171            |\n",
            "|    total_timesteps      | 77824          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0014924312   |\n",
            "|    clip_fraction        | 0.00796        |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.42          |\n",
            "|    explained_variance   | -0.156         |\n",
            "|    learning_rate        | 0.0001         |\n",
            "|    loss                 | 0.154          |\n",
            "|    n_updates            | 370            |\n",
            "|    policy_gradient_loss | -0.0038        |\n",
            "|    reward               | -0.00031344415 |\n",
            "|    std                  | 0.998          |\n",
            "|    value_loss           | 0.0365         |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 455           |\n",
            "|    iterations           | 39            |\n",
            "|    time_elapsed         | 175           |\n",
            "|    total_timesteps      | 79872         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0014557971  |\n",
            "|    clip_fraction        | 0.00435       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.77         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.0137       |\n",
            "|    n_updates            | 380           |\n",
            "|    policy_gradient_loss | -0.00268      |\n",
            "|    reward               | -0.0004278488 |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 0.00265       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 455           |\n",
            "|    iterations           | 40            |\n",
            "|    time_elapsed         | 179           |\n",
            "|    total_timesteps      | 81920         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0020145173  |\n",
            "|    clip_fraction        | 0.0112        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0.633         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | -0.000533     |\n",
            "|    n_updates            | 390           |\n",
            "|    policy_gradient_loss | -0.00227      |\n",
            "|    reward               | 2.4673461e-06 |\n",
            "|    std                  | 0.999         |\n",
            "|    value_loss           | 0.0946        |\n",
            "-------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 4: TD3"
      ],
      "metadata": {
        "id": "Kor83dZD-4AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Model: TD3\n",
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 128, \n",
        "              \"buffer_size\": 1000000, \n",
        "              \"learning_rate\": 0.0003}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "trained_td3 = agent.train_model(model=model_td3, \n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=30000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI4xEISawd01",
        "outputId": "b0d053ce-88a0-4c61-9952-9b4653e2ae24"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 1000000, 'learning_rate': 0.0003}\n",
            "Using cuda device\n",
            "day: 2515, episode: 70\n",
            "begin_total_asset: 100762.50\n",
            "end_total_asset: 862.32\n",
            "total_reward: -99900.18\n",
            "total_cost: 99278.64\n",
            "total_trades: 2515\n",
            "Sharpe: 0.058\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/              |              |\n",
            "|    episodes        | 4            |\n",
            "|    fps             | 109          |\n",
            "|    time_elapsed    | 91           |\n",
            "|    total_timesteps | 10064        |\n",
            "| train/             |              |\n",
            "|    actor_loss      | 15           |\n",
            "|    critic_loss     | 6.64e+03     |\n",
            "|    learning_rate   | 0.0003       |\n",
            "|    n_updates       | 7548         |\n",
            "|    reward          | 0.0007663914 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/              |              |\n",
            "|    episodes        | 8            |\n",
            "|    fps             | 97           |\n",
            "|    time_elapsed    | 206          |\n",
            "|    total_timesteps | 20128        |\n",
            "| train/             |              |\n",
            "|    actor_loss      | 11.5         |\n",
            "|    critic_loss     | 548          |\n",
            "|    learning_rate   | 0.0003       |\n",
            "|    n_updates       | 17612        |\n",
            "|    reward          | 0.0007663914 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/              |              |\n",
            "|    episodes        | 12           |\n",
            "|    fps             | 93           |\n",
            "|    time_elapsed    | 323          |\n",
            "|    total_timesteps | 30192        |\n",
            "| train/             |              |\n",
            "|    actor_loss      | 9.5          |\n",
            "|    critic_loss     | 33.2         |\n",
            "|    learning_rate   | 0.0003       |\n",
            "|    n_updates       | 27676        |\n",
            "|    reward          | 0.0007663914 |\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 5: SAC"
      ],
      "metadata": {
        "id": "EFiOemfG-75X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Model: SAC\n",
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "VuqYbvwi6A9Z"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trading\n",
        "We have trained five different models on our datasets now let’s trade using the environment class we inintialized above for creating trading environment, let’s assume that you are having $100K initial money on date 2019-01-01. We will use the TD3 trained model to trade AAPL."
      ],
      "metadata": {
        "id": "jpR2m4uO_AGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trade.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4QBBcb9M6A6l",
        "outputId": "ac29815f-8665-4def-e7a3-7f8ef6f1b92a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date       open       high        low      close     volume   tic  \\\n",
              "0  2019-01-02  38.722500  39.712502  38.557499  38.168346  148158800  AAPL   \n",
              "1  2019-01-03  35.994999  36.430000  35.500000  34.366505  365248800  AAPL   \n",
              "2  2019-01-04  36.132500  37.137501  35.950001  35.833580  234428400  AAPL   \n",
              "3  2019-01-07  37.174999  37.207500  36.474998  35.753815  219111200  AAPL   \n",
              "4  2019-01-08  37.389999  37.955002  37.130001  36.435398  164101200  AAPL   \n",
              "\n",
              "   day      macd    boll_ub    boll_lb     rsi_30      cci_30      dx_30  \\\n",
              "0    2 -1.999254  44.116378  35.134672  37.867349  -91.593349  42.250808   \n",
              "1    3 -2.180507  43.528024  34.692683  32.751921 -177.944136  55.246973   \n",
              "2    4 -2.180634  43.074805  34.458765  36.192793 -139.748808  47.060632   \n",
              "3    0 -2.162246  42.627003  34.259068  36.088933 -122.752210  46.245025   \n",
              "4    1 -2.068827  42.359778  34.097527  37.670001  -95.020349  37.537680   \n",
              "\n",
              "   close_30_sma  close_60_sma  \n",
              "0     40.865256     46.081709  \n",
              "1     40.451637     45.754132  \n",
              "2     40.148713     45.453094  \n",
              "3     39.914672     45.138282  \n",
              "4     39.704962     44.877020  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fcfbb4e7-97a0-4a90-97ca-613a1f581e47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>boll_ub</th>\n",
              "      <th>boll_lb</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>close_30_sma</th>\n",
              "      <th>close_60_sma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-01-02</td>\n",
              "      <td>38.722500</td>\n",
              "      <td>39.712502</td>\n",
              "      <td>38.557499</td>\n",
              "      <td>38.168346</td>\n",
              "      <td>148158800</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.999254</td>\n",
              "      <td>44.116378</td>\n",
              "      <td>35.134672</td>\n",
              "      <td>37.867349</td>\n",
              "      <td>-91.593349</td>\n",
              "      <td>42.250808</td>\n",
              "      <td>40.865256</td>\n",
              "      <td>46.081709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-01-03</td>\n",
              "      <td>35.994999</td>\n",
              "      <td>36.430000</td>\n",
              "      <td>35.500000</td>\n",
              "      <td>34.366505</td>\n",
              "      <td>365248800</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>3</td>\n",
              "      <td>-2.180507</td>\n",
              "      <td>43.528024</td>\n",
              "      <td>34.692683</td>\n",
              "      <td>32.751921</td>\n",
              "      <td>-177.944136</td>\n",
              "      <td>55.246973</td>\n",
              "      <td>40.451637</td>\n",
              "      <td>45.754132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-01-04</td>\n",
              "      <td>36.132500</td>\n",
              "      <td>37.137501</td>\n",
              "      <td>35.950001</td>\n",
              "      <td>35.833580</td>\n",
              "      <td>234428400</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>4</td>\n",
              "      <td>-2.180634</td>\n",
              "      <td>43.074805</td>\n",
              "      <td>34.458765</td>\n",
              "      <td>36.192793</td>\n",
              "      <td>-139.748808</td>\n",
              "      <td>47.060632</td>\n",
              "      <td>40.148713</td>\n",
              "      <td>45.453094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-01-07</td>\n",
              "      <td>37.174999</td>\n",
              "      <td>37.207500</td>\n",
              "      <td>36.474998</td>\n",
              "      <td>35.753815</td>\n",
              "      <td>219111200</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.162246</td>\n",
              "      <td>42.627003</td>\n",
              "      <td>34.259068</td>\n",
              "      <td>36.088933</td>\n",
              "      <td>-122.752210</td>\n",
              "      <td>46.245025</td>\n",
              "      <td>39.914672</td>\n",
              "      <td>45.138282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-01-08</td>\n",
              "      <td>37.389999</td>\n",
              "      <td>37.955002</td>\n",
              "      <td>37.130001</td>\n",
              "      <td>36.435398</td>\n",
              "      <td>164101200</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>1</td>\n",
              "      <td>-2.068827</td>\n",
              "      <td>42.359778</td>\n",
              "      <td>34.097527</td>\n",
              "      <td>37.670001</td>\n",
              "      <td>-95.020349</td>\n",
              "      <td>37.537680</td>\n",
              "      <td>39.704962</td>\n",
              "      <td>44.877020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fcfbb4e7-97a0-4a90-97ca-613a1f581e47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fcfbb4e7-97a0-4a90-97ca-613a1f581e47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fcfbb4e7-97a0-4a90-97ca-613a1f581e47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a prediction and get the account value change"
      ],
      "metadata": {
        "id": "r5QGTcLZ_McK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trade = data_split(data_df, start = '2019-01-01', end = '2021-01-01')\n",
        "e_trade_gym = StockTradingEnv(df = trade, **env_kwargs)\n",
        "env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
        "\n",
        "df_account_value, df_actions = DRLAgent.DRL_prediction(model=trained_td3, environment = env_trade)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "o4eK6xGX6A4I",
        "outputId": "b081aea1-c20a-40c0-dd01-adae37a1c379"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-c43da7963efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv_trade\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_trade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me_trade_gym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sb_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_account_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDRLAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDRL_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrained_td3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrade\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_trade\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_trade\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: DRL_prediction() got an unexpected keyword argument 'test_data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rpJVGZfP6A1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0YWEEti_6Ayw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dR8A7HEQ6AwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "waAE-UZB6AtS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}